{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11043003",
   "metadata": {
    "id": "11043003"
   },
   "source": [
    "# Federated Learning 적용: NCT-CRC-HE-100K 병리 이미지 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Fx6bSRSVZeyH",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fx6bSRSVZeyH",
    "outputId": "c5bfa391-0169-45fd-fa68-dd879223c13f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "AZB1kf28Z3NA",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "AZB1kf28Z3NA",
    "outputId": "38d8c1f4-8379-4a62-9d90-ff78e99e3d1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-06-09 17:10:27--  https://zenodo.org/record/1214456/files/NCT-CRC-HE-100K.zip?download=1\n",
      "Resolving zenodo.org (zenodo.org)... 188.185.43.25, 188.185.48.194, 188.185.45.92, ...\n",
      "Connecting to zenodo.org (zenodo.org)|188.185.43.25|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 MOVED PERMANENTLY\n",
      "Location: /records/1214456/files/NCT-CRC-HE-100K.zip [following]\n",
      "--2025-06-09 17:10:28--  https://zenodo.org/records/1214456/files/NCT-CRC-HE-100K.zip\n",
      "Reusing existing connection to zenodo.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 11690284003 (11G) [application/octet-stream]\n",
      "Saving to: ‘NCT-CRC-HE-100K.zip’\n",
      "\n",
      "NCT-CRC-HE-100K.zip   3%[                    ] 429.16M  27.7MB/s    eta 6m 48s ^C\n",
      "--2025-06-09 17:10:44--  https://zenodo.org/record/1214456/files/CRC-VAL-HE-7K.zip?download=1\n",
      "Resolving zenodo.org (zenodo.org)... 188.185.43.25, 188.185.48.194, 188.185.45.92, ...\n",
      "Connecting to zenodo.org (zenodo.org)|188.185.43.25|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 MOVED PERMANENTLY\n",
      "Location: /records/1214456/files/CRC-VAL-HE-7K.zip [following]\n",
      "--2025-06-09 17:10:45--  https://zenodo.org/records/1214456/files/CRC-VAL-HE-7K.zip\n",
      "Reusing existing connection to zenodo.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 800276929 (763M) [application/octet-stream]\n",
      "Saving to: ‘CRC-VAL-HE-7K.zip’\n",
      "\n",
      "CRC-VAL-HE-7K.zip    74%[=============>      ] 570.28M  24.6MB/s    eta 8s     "
     ]
    }
   ],
   "source": [
    "# 이미지 파일 다운\n",
    "# 1. NCT-CRC-HE-100K.zip -> train dataset (~11.7GB)\n",
    "!wget \"https://zenodo.org/record/1214456/files/NCT-CRC-HE-100K.zip?download=1\" -O NCT-CRC-HE-100K.zip\n",
    "\n",
    "# 2. CRC-VAL-HE-7K.zip -> test dataset (~800MB)\n",
    "!wget \"https://zenodo.org/record/1214456/files/CRC-VAL-HE-7K.zip?download=1\" -O CRC-VAL-HE-7K.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vkrwXs0CbENV",
   "metadata": {
    "id": "vkrwXs0CbENV"
   },
   "outputs": [],
   "source": [
    "# 압축 해제할 디렉토리 생성\n",
    "!mkdir -p ./data/NCT-CRC-HE-100K\n",
    "!mkdir -p ./data/CRC-VAL-HE-7K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "mgGXctq3bFXQ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mgGXctq3bFXQ",
    "outputId": "9a1e4793-5a8a-4198-b204-1dfb084529e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  NCT-CRC-HE-100K.zip\n",
      "replace ./data/NCT-CRC-HE-100K/NCT-CRC-HE-100K/ADI/ADI-AAAMHQMK.tif? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n",
      "N\n",
      "NCT-CRC-HE-100K\n",
      "Archive:  CRC-VAL-HE-7K.zip\n",
      "replace ./data/CRC-VAL-HE-7K/CRC-VAL-HE-7K/ADI/ADI-TCGA-AAICEQFN.tif? [y]es, [n]o, [A]ll, [N]one, [r]ename: CRC-VAL-HE-7K\n"
     ]
    }
   ],
   "source": [
    "# zip 파일 압축 해제\n",
    "!unzip NCT-CRC-HE-100K.zip -d ./data/NCT-CRC-HE-100K\n",
    "!ls ./data/NCT-CRC-HE-100K\n",
    "!unzip CRC-VAL-HE-7K.zip -d ./data/CRC-VAL-HE-7K\n",
    "!ls ./data/CRC-VAL-HE-7K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "UfimMSIobIew",
   "metadata": {
    "id": "UfimMSIobIew"
   },
   "outputs": [],
   "source": [
    "# 경로 설정\n",
    "original_train_root = './data/NCT-CRC-HE-100K/NCT-CRC-HE-100K'\n",
    "sorted_train_root = './data/NCT-CRC-HE-100K_sorted'\n",
    "original_val_root = './data/CRC-VAL-HE-7K/CRC-VAL-HE-7K'\n",
    "sorted_val_root = './data/CRC-VAL-HE-7K_sorted'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "AZ573_DXbQL5",
   "metadata": {
    "id": "AZ573_DXbQL5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import torch\n",
    "import shutil\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tHzHTn8ubJ5M",
   "metadata": {
    "id": "tHzHTn8ubJ5M"
   },
   "outputs": [],
   "source": [
    "# 기존 폴더 삭제 후 재생성\n",
    "shutil.rmtree(sorted_train_root, ignore_errors=True)\n",
    "shutil.rmtree(sorted_val_root, ignore_errors=True)\n",
    "os.makedirs(sorted_train_root, exist_ok=True)\n",
    "os.makedirs(sorted_val_root, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "UM7niFQZbLK4",
   "metadata": {
    "id": "UM7niFQZbLK4"
   },
   "outputs": [],
   "source": [
    "# 학습 데이터 복사\n",
    "for cls in sorted(os.listdir(original_train_root)):\n",
    "    src = os.path.join(original_train_root, cls)\n",
    "    dst = os.path.join(sorted_train_root, cls)\n",
    "    if os.path.isdir(src):\n",
    "        shutil.copytree(src, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "H_XYwRPybMSW",
   "metadata": {
    "id": "H_XYwRPybMSW"
   },
   "outputs": [],
   "source": [
    "# 검증 데이터 복사\n",
    "for cls in sorted(os.listdir(original_val_root)):\n",
    "    src = os.path.join(original_val_root, cls)\n",
    "    dst = os.path.join(sorted_val_root, cls)\n",
    "    if os.path.isdir(src):\n",
    "        shutil.copytree(src, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yJvXOABKbjNs",
   "metadata": {
    "id": "yJvXOABKbjNs"
   },
   "outputs": [],
   "source": [
    "# Train 폴더 Class 별 Image 개수\n",
    "def count_images_per_class(root_dir):\n",
    "    for cls in sorted(os.listdir(root_dir)):\n",
    "        cls_path = os.path.join(root_dir, cls)\n",
    "        if os.path.isdir(cls_path):\n",
    "            num_images = len([f for f in os.listdir(cls_path) if f.endswith('.tif')])\n",
    "            print(f\"{cls}: {num_images} images\")\n",
    "\n",
    "count_images_per_class('./data/NCT-CRC-HE-100K_sorted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-J9CPh40bkXp",
   "metadata": {
    "id": "-J9CPh40bkXp"
   },
   "outputs": [],
   "source": [
    "# Test 폴더 내 Class 별 Image 개수\n",
    "count_images_per_class('./data/CRC-VAL-HE-7K_sorted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HrBz7m0tb9b8",
   "metadata": {
    "id": "HrBz7m0tb9b8"
   },
   "outputs": [],
   "source": [
    "# Data Transform\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.05, contrast=0.05, saturation=0.05, hue=0.05),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tu5ZQGmMblbh",
   "metadata": {
    "id": "tu5ZQGmMblbh"
   },
   "source": [
    "#학습 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80004cc",
   "metadata": {
    "collapsed": true,
    "id": "c80004cc"
   },
   "outputs": [],
   "source": [
    "!pip install opacus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "IySzpc8irUD-",
   "metadata": {
    "id": "IySzpc8irUD-"
   },
   "outputs": [],
   "source": [
    "# 셀 1: 필수 라이브러리\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import copy\n",
    "from opacus import PrivacyEngine\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d367de0",
   "metadata": {
    "id": "8d367de0"
   },
   "outputs": [],
   "source": [
    "# CNN 모델 정의\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, num_classes=9):\n",
    "        super(CNNModel, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        # Replace BatchNorm with GroupNorm\n",
    "        self.gn1 = nn.GroupNorm(32, 32)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        # Replace BatchNorm with GroupNorm\n",
    "        self.gn2 = nn.GroupNorm(32, 64)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        # Replace BatchNorm with GroupNorm\n",
    "        self.gn3 = nn.GroupNorm(32, 128)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.dropout_conv = nn.Dropout2d(0.3)  # Conv 뒤 Dropout (feature map dropout)\n",
    "\n",
    "        # Calculate the flattened size based on the pooling layers\n",
    "        # Assuming input size of 224x224, after 3 MaxPool layers with kernel size 2 and stride 2,\n",
    "        # the spatial dimensions will be 224 / (2*2*2) = 224 / 8 = 28\n",
    "        self.fc1 = nn.Linear(128 * 28 * 28, 512)\n",
    "        self.dropout_fc = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.gn1(self.conv1(x))))\n",
    "        x = self.pool(F.relu(self.gn2(self.conv2(x))))\n",
    "        x = self.pool(F.relu(self.gn3(self.conv3(x))))\n",
    "        x = self.dropout_conv(x)\n",
    "\n",
    "        x = x.view(-1, 128 * 28 * 28)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout_fc(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# 모델 초기화\n",
    "# Use the original number of classes for the CNN model\n",
    "global_model = CNNModel(num_classes=9)\n",
    "\n",
    "# 모델 구조 출력\n",
    "print(global_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NROlAyVRr1Mj",
   "metadata": {
    "id": "NROlAyVRr1Mj"
   },
   "outputs": [],
   "source": [
    "# 셀 3: 로컬 모델 학습 함수\n",
    "def train_local_model(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    for data, target in dataloader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kw6ot897r4KZ",
   "metadata": {
    "id": "kw6ot897r4KZ"
   },
   "outputs": [],
   "source": [
    "# 셀 4: PrivacyEngine 적용 함수\n",
    "def make_private(model, dataloader, optimizer, noise_multiplier=1.0, max_grad_norm=1.0):\n",
    "    privacy_engine = PrivacyEngine()\n",
    "    model, optimizer, dataloader = privacy_engine.make_private(\n",
    "        module=model,\n",
    "        optimizer=optimizer,\n",
    "        data_loader=dataloader,\n",
    "        noise_multiplier=noise_multiplier,\n",
    "        max_grad_norm=max_grad_norm\n",
    "    )\n",
    "    return model, optimizer, dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32hAnN2Xr5qI",
   "metadata": {
    "id": "32hAnN2Xr5qI"
   },
   "outputs": [],
   "source": [
    "# 셀 5: Federated Averaging 함수\n",
    "def average_weights(models):\n",
    "    avg_model = copy.deepcopy(models[0])\n",
    "    for key in avg_model.keys():\n",
    "        for i in range(1, len(models)):\n",
    "            avg_model[key] += models[i][key]\n",
    "        avg_model[key] = avg_model[key] / len(models)\n",
    "    return avg_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Hw7YrNN5r8zK",
   "metadata": {
    "id": "Hw7YrNN5r8zK"
   },
   "outputs": [],
   "source": [
    "# 셀 6: Federated Learning 루프\n",
    "def federated_learning(global_model, hospitals_dataloaders, device, rounds=5):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    global_model.to(device)\n",
    "\n",
    "    for r in range(rounds):\n",
    "        print(f\"--- Federated Round {r+1} ---\")\n",
    "        local_weights = []\n",
    "\n",
    "        for i, dataloader in enumerate(hospitals_dataloaders):\n",
    "            print(f\"Training on hospital {i+1}\")\n",
    "            local_model = copy.deepcopy(global_model)\n",
    "            optimizer = optim.SGD(local_model.parameters(), lr=0.01)\n",
    "\n",
    "            # DP 적용\n",
    "            model_dp, optimizer_dp, dataloader_dp = make_private(\n",
    "                local_model, dataloader, optimizer,\n",
    "                noise_multiplier=1.0, max_grad_norm=1.0\n",
    "            )\n",
    "\n",
    "            trained_model = train_local_model(model_dp, dataloader_dp, criterion, optimizer_dp, device)\n",
    "            # Get the state dictionary from the unwrapped module\n",
    "            local_weights.append(copy.deepcopy(trained_model.module.state_dict()))\n",
    "\n",
    "        avg_weights = average_weights(local_weights)\n",
    "        global_model.load_state_dict(avg_weights)\n",
    "\n",
    "    return global_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MMRACyHKr-cj",
   "metadata": {
    "id": "MMRACyHKr-cj"
   },
   "outputs": [],
   "source": [
    "# 셀 7: 병원별 데이터 생성 (더미 예시)\n",
    "input_dim = 20\n",
    "num_classes = 2\n",
    "hospital_datasets = []\n",
    "\n",
    "num_hospitals = 2  # 병원 수\n",
    "num_data_per_hospital = 30  # 각 병원당 데이터 수\n",
    "# Assuming image data with 3 channels (RGB) and height/width of 224x224 for the CNN model\n",
    "image_height = 224\n",
    "image_width = 224\n",
    "num_channels = 3\n",
    "\n",
    "for i in range(num_hospitals):  # 병원 3개\n",
    "    # Generate dummy image data with shape (batch_size, channels, height, width)\n",
    "    X = torch.randn(num_data_per_hospital, num_channels, image_height, image_width)\n",
    "    y = torch.randint(0, num_classes, (num_data_per_hospital,))\n",
    "    dataset = TensorDataset(X, y)\n",
    "    loader = DataLoader(dataset, batch_size=10, shuffle=True)\n",
    "    hospital_datasets.append(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MQoHKFSNsAAl",
   "metadata": {
    "id": "MQoHKFSNsAAl"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "# 셀 8: 학습 실행\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "global_model = CNNModel(num_classes=num_classes)\n",
    "\n",
    "# Federated Learning 수행\n",
    "global_model = federated_learning(global_model, hospital_datasets, device, rounds=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355ffe55",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 실제 학습 데이터셋 로딩 (ImageFolder)\n",
    "train_dataset = ImageFolder(root=sorted_train_root, transform=train_transform)\n",
    "val_dataset = ImageFolder(root=sorted_val_root, transform=val_transform)\n",
    "\n",
    "print(f\"전체 학습 이미지 수: {len(train_dataset)}\")\n",
    "print(f\"전체 검증 이미지 수: {len(val_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57154ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 병원 수 설정\n",
    "num_hospitals = 3\n",
    "data_per_hospital = len(train_dataset) // num_hospitals\n",
    "lengths = [data_per_hospital] * (num_hospitals - 1) + [len(train_dataset) - data_per_hospital * (num_hospitals - 1)]\n",
    "\n",
    "# 데이터 병원별 분할\n",
    "hospital_subsets = torch.utils.data.random_split(train_dataset, lengths)\n",
    "hospital_dataloaders = [DataLoader(subset, batch_size=32, shuffle=True) for subset in hospital_subsets]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25be9426",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# GPU/CPU 확인\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "global_model = CNNModel(num_classes=len(train_dataset.dataset.classes))\n",
    "\n",
    "# Federated Learning 수행 (차등 프라이버시 적용 포함)\n",
    "global_model = federated_learning(global_model, hospital_dataloaders, device, rounds=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b701a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 모델 평가 함수\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in dataloader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            pred = output.argmax(dim=1)\n",
    "            correct += (pred == target).sum().item()\n",
    "            total += target.size(0)\n",
    "    acc = correct / total\n",
    "    print(f\"Validation Accuracy: {acc * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5aefc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "evaluate(global_model, val_loader, device)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
